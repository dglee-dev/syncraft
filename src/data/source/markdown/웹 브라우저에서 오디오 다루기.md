---
title: 웹 브라우저에서 오디오 다루기
category: tech
---


# 웹 브라우저에서 오디오 다루기

이번 프로젝트를 진행하면서 오디오를 다루어야 했다.
버튼을 누를때, 미션이 완료되었을 때, 음악을 재생할 때..

오디오가 등장하는 순간이 수없이 많았고, 더 문제는 이것들을 클라이언트 환경에 미리 가져와두고 오프라인 환경에서 재생해야 했다.

처음에는 html head에 link preload를 사용도 해보고, indexedDB에 미리 다운로드 해 두기도 해 보았지만 여러가지 호환성/사용성 문제가 존재했다. GPT가 추천해 준 방법은 AudioContext API를 사용하는 방법이다.

속는셈치고 시도해 보았는데 잘 작동했고 여러가지 iOS 버전의 아이패드에서도 문제없이 작동했다. 프론트엔드 개발자에게는 호환성이 생명이다.

간단하게 AudioContext가 무엇인지, 어떻게 작동하는지, 어떻게 미리 오디오 데이터를 불러온 후 오프라인 환경에서 재생할 수 있게 만들었는지 알아보려고 한다.

## [AudioContext?](https://developer.mozilla.org/en-US/docs/Web/API/AudioContext)

> The AudioContext interface represents an audio-processing graph built from audio modules linked together, each represented by an AudioNode.

먼저 읽어보자. 오디오 컨텍스트 인터페이스는 오디오 노드들이 연결된 하나의 '오디오 프로세싱 그래프' 이다.

컨텍스트라는 말 그대로, 오디오 컨텍스트는 오디오를 프로세싱할 수 있는 하나의 '문맥' 이다. 이 문맥 안에서 오디오를 가공하고 재생할 수 있는 것이다.

> An audio context controls both the creation of the nodes it contains and the execution of the audio processing, or decoding. You need to create an AudioContext before you do anything else, as everything happens inside a context. It's recommended to create one AudioContext and reuse it instead of initializing a new one each time, and it's OK to use a single AudioContext for several different audio sources and pipeline concurrently.

하나의 오디오 컨텍스트는 그가 포함하고 있는 노드의 생성과 오디오 프로세싱-혹은 디코딩을 제어한다.

오디오 프로세싱을 진행하기 위해 매번 새로운 오디오 컨텍스트를 생성하기 보다 하나를 재사용하는 방식을 추천한다. 여러 다른 오디오 소스들을 동일한 오디오 컨텍스트로 처리하는 것이 나쁘지 않은 방식이라고 한다.

뭐야, 그럼 언제 새로운 오디오 컨텍스트를 사용하는게 좋은걸까? 라는 의문에 인공지능에게 물어보았다.

<img src="https://justin-cms-images.s3.ap-northeast-2.amazonaws.com/2024_01_01_9_18_31_8522f99498.png">

인공지능 녀석은 3가지 케이스를 알려주었는데, 그 중 가장 첫번째로 등장한 것은 아래와 같다.

>  If your application has a user interface where users can start and stop audio playback, you might consider creating a new AudioContext when the user initiates a new session or playback. This helps isolate audio contexts for different user interactions.

유저와의 인터랙션이 존재할 때 - 예를 들어 오디오 플레이백을 조절할 때(재생/정지)
각각의 인터랙션을 독립된 컨텍스트에서 다룰 수 있도록 돕는다고 한다.

새로운 프로젝트에서도 유저의 버튼 클릭에 따라 음악을 끄고 켜는 기능이 필요했고, 확실히 기존의 컨텍스트를 재활용하는 방식보다 완전히 컨텍스트를 종료해버리고 새로운 컨텍스트를 만들어 사용하는 것이 안정적으로 작동했다.

새로운 컨텍스트를 만들어 사용할 때 주의할 점은 기존의 컨텍스트를 확실하게 종료하고 해당 컨텍스트에서 사용하던 리소스를 완전히 클린업 시켜주었는지 확인해야 한다.

## Audio context in code

이제 wav 파일을 어떻게 오디오 컨텍스트에 로딩해 재생할 수 있는지 코드로 살펴보자.

```
const AudioContext = window.AudioContext || window.webkitAudioContext;
const audioContext = new AudioContext();

async function playWavFile (url) {
  // 1. arrayBuffer 형태로 wav 파일 데이터를 가져온다.
  const response = await fetch(url);
  const arrayBuffer = await response.arrayBuffer();
  
  // 2. decodeAudioData는 arrayBuffer 형태로 로딩된 오디오 데이터를 디코딩하여 `AudioBuffer` 형태로 만들어준다.
  const audioBuffer = await audioContext.decodeAudioData(arrayBuffer);
  
  // 3. AudioBufferSourceNode 
  const source = audioContext.createBufferSource();
  
  source.buffer = audioBuffer;
  source.connect(audioContext.destination);
  
  // start playing
  source.start();
}
```

코드에 등장한 개념들을 하나씩 살펴보자.

### [decodeAudioData](https://developer.mozilla.org/en-US/docs/Web/API/BaseAudioContext/decodeAudioData)

decodeAudioData 메서드는 쉽게 말해 'array' buffer를 'audio' buffer로 바꾸어주는 역할을 한다. 이 과정은 오디오 트랙을 Web Audio API에 적합한 오디오 소스로 변환시켜주기 위한 것이다.

### [createBufferSource](https://developer.mozilla.org/en-US/docs/Web/API/BaseAudioContext/createBufferSource)

decodeAudioData 등으로 디코딩 된 audio buffer를 재생시키려면 오디오버퍼 소스노드가 필요하다. `createBufferSource` 는 `AudioBufferSourceNode` 를 반환한다. 코드에서는 createBufferSource를 통해 오디오버퍼 소스노드를 생성하고 (source로 표현), buffer 프로퍼티에 오디오버퍼를 탑재하고, 오디오 컨텍스트 데스티네이션에 연결한 다음, 마지막으로 재생하는 과정을 거치고 있다.

도대체 AudioBufferSourceNode 라는 것은 무엇이고 어떻게 사용하는 것인가?

### [AudioBufferSourceNode](https://developer.mozilla.org/en-US/docs/Web/API/AudioBufferSourceNode)

> The `AudioBufferSourceNode` interface is an `AudioScheduledSourceNode` which represents an audio source consisting of in-memory audio data, stored in an `AudioBuffer`.

`AudioBufferSourceNode`는 또한 `AudioScheduledSourceNode`를 상속하는 클래스이다. `AudioScheduledSourceNode` 는, 디코딩 된 `AudioBuffer` 에 담긴 오디오 데이터들을 *메모리에* 저장하는 역할을 하는 인터페이스이다.

> This interface is especially useful for playing back audio which has particularly stringent timing accuracy requirements, such as for sounds that must match a specific rhythm and can be kept in memory rather than being played from disk or the network.

중요한 내용이다. 오디오버퍼 소스노드를 사용함으로서, 디스크나 네트워크를 통해서 로딩하는 것이 아닌 메모리에 사운드 데이터를 저장해둘 수 있고 레이턴시를 최소화 하여 정확히 원하는 타이밍에 오디오를 재생할 수 있다. 어쩌면 이것이 이번 프로젝트에 오디오 컨텍스트를 사용한 오디오 재생을 적용하는 것이 가장 적합했던 이유가 될 수 있겠다.

브라우저에서 딜레이없이 오디오를 재생해야하거나 프리로딩이 필요하다면 오디오 컨텍스트를 이용하라.